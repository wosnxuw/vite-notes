# 常见名词解释

这篇文章主要是记录大模型本身相关的名词

#### sbert Sentence-BERT

将句子或短文转换成固定长度的向量，这个向量能够很好地捕捉句子的语义信息

SBERT 本身是于 2019 年提出一个方法

不过后续逐渐成为后续模型的训练方法

在它出现之前，像 BERT 这样的模型虽然强大，但直接用于计算句子相似度效率很低

#### sbert 和 BGE-M3 的关系

BGE-M3 采用更先进的预训练模型（如 RoBERTa、ERNIE、DeBERTa）作为基座

"用 SBERT 做嵌入"，在实践中可能就是用 BGE、E5、GTE 等基于相同原理的现代模型

#### 向量长度

对一个给定的具体模型（例如 BGE-M3-zh 或 all-MiniLM-L6-v2）

它输出的所有句子向量长度都是完全相同的固定长度

无论输入句子是 "你好"，还是 "今天天气很好，我打算去公园散步并且晚上看一场电影"

模型都会输出相同维度的向量（例如 768 维或 1024 维）

即：我们需要将所有句子映射到同一个向量空间中

#### 如何做好的嵌入？

1、一份格式化的数据，比如 json、xml
2、只是把最关键数据罗列出来，但是并没有格式化
3、数据尚未整理，比较原始，但是按照语义进行了分段

似乎第 3 种才是最好的

准备数据的关键是创造“语义上自包含”的文本块。

1、
字段名、标点符号等结构性信息会“污染”语义
模型会学习到 "product_name": 和 "price": 这些模式的相似性，而不是产品本身的语义

2、
信息点孤立，缺乏连贯的语义上下文

3、
最接近模型训练时的数据分布

#### Token 消耗量

无论是 API，还是没有 token 概念的网页版

多轮对话的消耗都是 **A+2A+3A+...**

每次都会把之前的所有对话内容拼在一起

每次对话请求（包括多轮对话中的每一轮）都是对同一个“无状态”模型的一次独立调用

模型本身没有任何记忆，所谓的“连贯对话”完全依赖于：把之前的对话历史作为文本，重新输入给模型

这就像你每次问一个没有记忆的天才朋友问题，但你每次都把之前的聊天记录打印出来递给他看，他看完后回答你，他看起来“记得”，其实只是“刚读完”

历史记录管理属于上下文工程方面

根本原因：

大语言模型（如 GPT、LLaMA）在部署后，其参数是完全固定的，它的行为可以看作一个确定性函数。（不考虑量化后以及浮点数精度的问题，纯理论上）

没有“会话 ID”、没有“用户档案”、没有“上次聊了什么”的内部变量。

#### 使用函数时的 Token 消耗量

如果大模型调用外部函数，则 Token 用量飞速增长

如果我没搞错，一旦调用外部函数，模型就会停止（模型没有运行时实时获取外部函数返回值的功能）

模型给函数传参后，就停下，外部函数算好后，运行一个新的模型实例

虽然各种库给你封装的看起来是一次，但是实际上 Token 消耗很多的

如果你调用 10 次 20 次 才能解决问题，用量很大，这也可能是为什么，在 IDE 里这种 Agent，来回读你文件夹的它不免费

假设：用户问题消耗为 $A$ 个 token。模型生成的工具调用指令为 $B$ 个 token。外部工具返回的结果为 $C$ 个 token。总消耗情况：第一次推断：消耗 $A$。第二次推断：消耗 $A + B + C$。最终总计：$2A + B + C$。

### 多模态

#### 函数式多模态

其实就是外挂一个函数

本身它还是纯语言模型，你让它画图，它就帮你调用工具

#### 原生多模态

一些强大的模型 GPT 或者 Gemini 之类似乎训练时，已经不只是学习语言了，它也可以直接看懂一个图

你给它一个图，它不需要外挂一个 openCV 程序来识别猫狗，它直接就能识别猫狗

图片对于大模型来说 Token 消耗量很高，视频音频按照秒切分好像是

高清图片重复对话 Token 滚雪球

#### CLIP

最开始的多模态模型，是 OpenAI 的多模态模型 CLIP

因为传统文本模型，需要将词映射到语义空间中，即猫狗的词向量接近

而你狗的图片（即使你试图做一些编码）也无法直接加入到这个语义空间

所以 OpenAI 找了 4 亿个高质量图片，配上文本，这是一个 图片-文本 的键值对

通过对键值对进行对比学习，在语义空间做对齐，可以理解为将图片理解为文字

但是，描述图片的文字无法描述所有信息（谁高谁低，谁是什么颜色，有几个，谁大谁小），所以最后导致它识别传统 MNIST 数据集都只有 88 的准确率

它这个学习是双向的，它即能学习到狗图片和狗的相似性，也可以输入狗，找到最相似的图片

### 多头注意力

可以把“头”想象成一个委员会中的多个专家

每个专家（头）从自己的专业角度（子空间）分析同一个问题（输入序列），最后大家把意见汇总（拼接+线性变换），做出综合判断。一个头可能关注句法结构，另一个头可能关注语义角色，还有头可能关注局部或长距离依赖。

多头注意力机制本身并不是大模型能处理更长上下文的直接原因，但它为高效建模长距离依赖提供了基础。
真正让大模型能处理更长上下文的，是后续对注意力机制的改进（如稀疏注意力、位置编码优化、计算效率提升等）以及更大的模型规模和训练数据。

标准的 Transformer 使用的是全注意力（full attention），即每个 token 都要和其他所有 token 计算注意力权重。
因此，早期的 Transformer 模型（如 BERT、GPT-2）通常只支持 512 或 1024 个 token 的上下文。

自注意力机制：可直接建模任意两个 token 之间的关系

每个 token 都能直接关注序列中任意其他 token。

无论距离多远（第 1 个词和第 1000 个词），都能建立直接连接。

特别适合理解句子中的主谓一致、指代消解等长程语义。

它用“注意力”替代了“递归”或“卷积”

思考：你说“回到开头”，模型真的会关注开头吗？

#### 无法数清楚 Strawberry 有几个 r

因为大模型是分词，并不是分字母，它的理解局限于 Token 层级，目的是预测下一个 Token，所以没有关注一个 Token 里有几个 r

但是如果你叫它一步步思考，确实可能会成功

就如同 deepseek 说的，Transformer 的“随意注意”为模型提供了全局的、语义的上下文视野，但这双“眼睛”是为瞭望山川、识别物体而生的，而不是用来数清脚下沙子的精确数目

即，Transformer 给了大模型一个可以看到全局的眼睛，但是并不是一个同时处理所有事情的眼睛。就好比人，你虽然能听到附近的所有声音，但是过于微小的（在大模型眼里就是 r 有几个），或者信息太乱，也会忘掉

大模型训练的时候，总是关注附近的词（说是有什么中间遗忘，开头结尾比较好记住），这样分高。导致你和他对话的时候，他虽然看到了，但是还是会忘掉

#### Stable Diffusion 模型

它有三个部分

（1）文本编码器 CLIP。这里直接是 OpenAI 的 CLIP，这样，可以让输入的文字，在语义空间对齐
（2）扩散模型 Diffusion。基本上扩散模型出来以后，就替代了 GAN。扩散模型的目标是对一个噪声图片逐渐去噪，使得它逼近目标。它内部的实现，以前是 UNet，现在是 Transformer，后者称之为 DiT
（3）图像解码器 VAE。类似于超分，减轻中间扩散模型的压力，让它的输出不至于太大

训练时，准备文本-图片对，然后对图片逐步添加噪音，用 Diffusion 学习如何去噪

#### 分数测试清单

HLE：Humanity's Late Exam 人类的最后一次考试，这是一个专门设计的超级困难的问题集合，持续更新

LiveCodeBench：一些难的代码题，持续更新

SWE：工程能力，看大模型能否提交 github 上的一个 pr，通过 test suit

#### RVLR

 Recurrent Vector Long-term Retrieval（循环向量长期检索），这是当前增强大型语言模型（LLMs）处理超长上下文能力的一项前沿技术

RMT (Recurrent Memory Transformer)：明确提出了递归记忆令牌的概念，是 RVLR 思想的直接体现

#### RLHF
RLHF（Reinforcement Learning from Human Feedback），即基于人类反馈的强化学习，是一套复杂的训练技术，其核心目标是让 AI 模型的行为与人类的价值观和意图对齐

包括遵循指令的能力，也是靠 RLHF